# GEO Tracking - Realistic Implementation Plan

## Executive Summary

Dette dokument beskriver en **realistisk og implementerbar** version af GEO (Generative Engine Optimization) tracking, baseret pÃ¥ jeres nuvÃ¦rende tech stack og ressourcer.

**MÃ¥l:** Track om dit website bliver citeret i AI-genererede sÃ¸geresultater.

**Scope:** Fokus pÃ¥ det der faktisk kan implementeres og give vÃ¦rdi NU, uden at bruge 6+ mÃ¥neder.

---

## ğŸ¯ Realistic MVP - Hvad vi FAKTISK kan bygge

### Core Features (2-3 uger development)

1. âœ… **Manual Query Testing**
   - Bruger indtaster en query manuelt
   - KÃ¸rer den gennem ChatGPT, Claude, Perplexity
   - Viser om deres site er citeret

2. âœ… **Citation Detection**  
   - URL matching i AI response
   - Domain name detection
   - Simple visibility score

3. âœ… **Top Queries from GSC Import**
   - Hent top 50-100 queries fra Google Search Console
   - Filter for informational queries (simpel regex)
   - Lad bruger vÃ¦lge hvilke de vil tracke

4. âœ… **Competitor Tracking**
   - Vis hvilke andre domains der citeres
   - Simpel ranking efter antal citationer

5. âœ… **Historical Tracking**
   - Gem results over tid
   - Vis trend (cited mere eller mindre?)

---

## ğŸš€ Phase 1: Minimal Viable Product (Week 1-3)

### What We Build

#### **1. GEO Tab UI**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GEO TRACKING                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ Test a Query:                                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚ â”‚ Enter query: [hvad er SEO____________]    â”‚          â”‚
â”‚ â”‚                                            â”‚          â”‚
â”‚ â”‚ Test on:                                   â”‚          â”‚
â”‚ â”‚ â˜‘ ChatGPT  â˜‘ Claude  â˜‘ Perplexity         â”‚          â”‚
â”‚ â”‚                                            â”‚          â”‚
â”‚ â”‚ [Test Now] [Save & Track]                  â”‚          â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TRACKED QUERIES (5)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ â€¢ "hvad er tommer til cm"         Last: 2h ago         â”‚
â”‚   ChatGPT: âœ“ Cited    Claude: âœ— Not cited             â”‚
â”‚   [View Details] [Re-test]                              â”‚
â”‚                                                         â”‚
â”‚ â€¢ "hvordan omregner man tommer"   Last: 1d ago         â”‚
â”‚   ChatGPT: âœ— Not cited    Claude: âœ“ Cited             â”‚
â”‚   [View Details] [Re-test]                              â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **2. Simple API Integration**

**Only 2 engines for MVP:**
- âœ… ChatGPT (via OpenAI API)
- âœ… Claude (via Anthropic API)

**Skip for MVP:**
- âŒ Perplexity (kan tilfÃ¸jes senere)
- âŒ Google Gemini (for komplekst)
- âŒ Fan-out query tracking (Phase 2 feature)

#### **3. Database Schema (Simplified)**

```sql
-- Tracked Queries
CREATE TABLE geo_queries (
    id SERIAL PRIMARY KEY,
    site_url VARCHAR(500) NOT NULL,
    query TEXT NOT NULL,
    active BOOLEAN DEFAULT TRUE,
    priority INTEGER DEFAULT 1,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(site_url, query)
);

-- Test Results
CREATE TABLE geo_test_results (
    id SERIAL PRIMARY KEY,
    query_id INTEGER REFERENCES geo_queries(id),
    engine VARCHAR(50) NOT NULL, -- 'chatgpt' or 'claude'
    response_text TEXT,
    was_cited BOOLEAN DEFAULT FALSE,
    citation_count INTEGER DEFAULT 0,
    visibility_score INTEGER DEFAULT 0,
    competitors_found TEXT[], -- Array of competitor domains
    tested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX(query_id, engine, tested_at)
);

-- Citations Found
CREATE TABLE geo_citations (
    id SERIAL PRIMARY KEY,
    test_result_id INTEGER REFERENCES geo_test_results(id),
    domain VARCHAR(255) NOT NULL,
    url TEXT,
    is_user_domain BOOLEAN DEFAULT FALSE,
    position_in_response INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### **4. API Endpoints**

**Endpoint 1: Test Query**
```
POST /api/geo/test-query

Body:
{
  "query": "hvad er SEO",
  "engines": ["chatgpt", "claude"],
  "userDomain": "omregne.dk"
}

Response:
{
  "results": [
    {
      "engine": "chatgpt",
      "cited": true,
      "visibilityScore": 65,
      "position": 2,
      "competitors": ["semrush.com", "moz.com"],
      "responseExcerpt": "..."
    },
    {
      "engine": "claude",
      "cited": false,
      "competitors": ["ahrefs.com", "semrush.com"]
    }
  ]
}
```

**Endpoint 2: Get Tracked Queries**
```
GET /api/geo/queries?siteUrl=https://omregne.dk

Response:
{
  "queries": [
    {
      "id": 1,
      "query": "hvad er tommer til cm",
      "lastTested": "2025-11-07T14:00:00Z",
      "results": {
        "chatgpt": { "cited": true, "score": 75 },
        "claude": { "cited": false, "score": 0 }
      }
    }
  ]
}
```

**Endpoint 3: Import from GSC**
```
POST /api/geo/import-gsc-queries

Body:
{
  "siteUrl": "https://omregne.dk",
  "startDate": "2025-10-01",
  "endDate": "2025-11-01",
  "minImpressions": 100
}

Response:
{
  "imported": 47,
  "suggestions": [
    { "query": "hvad er...", "impressions": 1234 },
    { "query": "hvordan...", "impressions": 987 }
  ]
}
```

#### **5. Citation Extraction (Simplified)**

**Simple URL/Domain Matching:**

```javascript
function extractCitations(responseText, userDomain) {
  const citations = [];
  
  // Method 1: Find all URLs
  const urlRegex = /https?:\/\/[^\s<>"]+/g;
  const urls = responseText.match(urlRegex) || [];
  
  urls.forEach(url => {
    try {
      const domain = new URL(url).hostname.replace('www.', '');
      citations.push({
        url,
        domain,
        isUser: domain === userDomain,
      });
    } catch (e) {}
  });
  
  // Method 2: Find domain mentions (e.g., "according to omregne.dk")
  const domainRegex = new RegExp(`\\b${userDomain}\\b`, 'gi');
  const mentions = responseText.match(domainRegex) || [];
  
  return {
    cited: citations.some(c => c.isUser) || mentions.length > 0,
    citationCount: citations.filter(c => c.isUser).length + mentions.length,
    allCitations: citations,
    competitors: [...new Set(citations.filter(c => !c.isUser).map(c => c.domain))],
  };
}
```

---

## ğŸ’¡ What We SKIP for MVP (Can Add Later)

### âŒ Not in MVP:
1. **Fan-out query tracking** - Too complex, API costs 3-6x higher
2. **Automated daily execution** - Start with manual/on-demand
3. **Multiple languages** - Start with Danish/English only
4. **Gemini/Perplexity** - Focus on ChatGPT + Claude first
5. **Advanced analytics** - Just show basic citation yes/no
6. **Email notifications** - Manual check for now
7. **Query expansion** - Manual query entry only
8. **LLM-based citation extraction** - Simple regex first

### âœ… Can Add in Phase 2 (if MVP successful):
- Scheduled automated testing (daily/weekly)
- More AI engines (Perplexity, Gemini)
- Better citation extraction using LLM
- Fan-out query detection (ChatGPT + Claude)
- Advanced competitor analysis
- Email alerts

---

## ğŸ› ï¸ Technical Implementation

### **Stack:**
- Frontend: React/Next.js (already in use)
- Backend: Next.js API routes
- Database: JSON file storage (same as annotations) OR upgrade to real DB
- APIs: OpenAI + Anthropic

### **Required Dependencies:**

```bash
npm install openai @anthropic-ai/sdk
```

### **Environment Variables:**

```env
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
```

### **Code Structure:**

```
app/
  dashboard/
    geo/
      page.tsx              # Main GEO dashboard
  api/
    geo/
      test-query/
        route.ts            # Test a single query
      queries/
        route.ts            # CRUD for tracked queries
      import-gsc/
        route.ts            # Import queries from GSC
      
components/
  GEOQueryTester.tsx        # UI for testing queries
  GEOResultsTable.tsx       # Display test results
  GEOMetricsCards.tsx       # Show overall stats
  
lib/
  geoTracking.ts            # Citation extraction logic
  geoStorage.ts             # Store/retrieve results (JSON or DB)
```

---

## ğŸ“Š Realistic Feature Set

### **What User Can Do:**

1. **Test Individual Queries:**
   - Type in "hvad er tommer"
   - Click "Test Now"
   - See results from ChatGPT and Claude in 5-10 seconds
   - See if omregne.dk is mentioned
   - See what competitors are mentioned instead

2. **Save Queries for Tracking:**
   - Click "Save & Track"
   - Query appears in "Tracked Queries" list
   - Can manually re-test anytime

3. **Import Top GSC Queries:**
   - Click "Import from Search Console"
   - System fetches top informational queries
   - User selects which ones to track
   - Can test all at once

4. **View Results:**
   - Simple table showing:
     - Query text
     - ChatGPT: âœ“/âœ—
     - Claude: âœ“/âœ—
     - Last tested timestamp
     - Competitors found

5. **Track Over Time:**
   - Re-test same query weekly
   - See if citation status changed
   - Simple trend indicator (â†‘ improving, â†“ declining, â†’ stable)

### **What System Shows:**

#### **Metrics (Simple):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Citation Rate                      â”‚
â”‚                                    â”‚
â”‚ ChatGPT:  3/10 queries (30%)      â”‚
â”‚ Claude:   5/10 queries (50%)      â”‚
â”‚                                    â”‚
â”‚ Overall:  8/20 tests (40%)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Top Competitors:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Most Cited Competitors             â”‚
â”‚                                    â”‚
â”‚ 1. wikipedia.org     - 12 times   â”‚
â”‚ 2. semrush.com       - 8 times    â”‚
â”‚ 3. ahrefs.com        - 6 times    â”‚
â”‚ 4. omregne.dk        - 5 times    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’° Cost Estimation (Realistic)

### **API Costs:**

**Conservative Usage:**
- 20 tracked queries
- Test each query 2x/week (on both engines)
- = 40 tests/week per engine
- = 160 tests/month per engine

**Monthly Cost:**
- ChatGPT: 160 Ã— $0.03 = **$4.80/month**
- Claude: 160 Ã— $0.015 = **$2.40/month**
- **Total: ~$7-10/month** (very affordable!)

**If user tests manually often:**
- Up to $20-30/month (still reasonable)

### **Development Time:**

**Week 1:**
- Database schema
- API endpoints for testing
- Basic citation extraction

**Week 2:**
- GEO tab UI
- Query testing interface
- Results display

**Week 3:**
- GSC query import
- Historical tracking
- Polish & bug fixes

**Total: 2-3 weeks for working MVP**

---

## ğŸ¨ UI Components (Simplified)

### **Component 1: Query Tester**

```typescript
// Simple form for testing queries
interface QueryTesterProps {
  siteUrl: string;
}

Features:
- Text input for query
- Checkboxes for ChatGPT/Claude
- "Test Now" button
- Loading state
- Results display
```

### **Component 2: Results Display**

```typescript
interface QueryResult {
  query: string;
  engine: 'chatgpt' | 'claude';
  cited: boolean;
  visibilityScore: number;
  competitors: string[];
  responseExcerpt: string;
  testedAt: string;
}

// Show results in a clean table or card layout
```

### **Component 3: Tracked Queries List**

```typescript
// Simple table showing:
- Query text
- Last tested
- ChatGPT status (âœ“/âœ—)
- Claude status (âœ“/âœ—)
- Actions (Re-test, View details, Delete)
```

---

## ğŸ”§ Implementation Details

### **Step 1: API Integration**

**ChatGPT Integration:**

```typescript
// app/api/geo/test-query/route.ts

import { OpenAI } from 'openai';

async function testQueryOnChatGPT(query: string, userDomain: string) {
  const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  });

  const response = await openai.chat.completions.create({
    model: 'gpt-4o',
    messages: [
      {
        role: 'user',
        content: query,
      },
    ],
    temperature: 0.3,
  });

  const responseText = response.choices[0].message.content || '';
  
  // Extract citations
  const citations = extractCitations(responseText, userDomain);
  
  return {
    engine: 'chatgpt',
    responseText,
    cited: citations.cited,
    citationCount: citations.citationCount,
    visibilityScore: calculateScore(citations),
    competitors: citations.competitors,
  };
}
```

**Claude Integration:**

```typescript
import Anthropic from '@anthropic-ai/sdk';

async function testQueryOnClaude(query: string, userDomain: string) {
  const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
  });

  const message = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 2000,
    messages: [
      {
        role: 'user',
        content: query,
      },
    ],
  });

  const responseText = message.content[0].type === 'text' 
    ? message.content[0].text 
    : '';
  
  const citations = extractCitations(responseText, userDomain);
  
  return {
    engine: 'claude',
    responseText,
    cited: citations.cited,
    citationCount: citations.citationCount,
    visibilityScore: calculateScore(citations),
    competitors: citations.competitors,
  };
}
```

### **Step 2: Citation Extraction (Keep It Simple)**

```typescript
// lib/geoTracking.ts

export function extractCitations(responseText: string, userDomain: string) {
  const allDomains: string[] = [];
  let userCitationCount = 0;
  
  // Extract all URLs
  const urlRegex = /https?:\/\/[^\s<>"'\)]+/g;
  const urls = responseText.match(urlRegex) || [];
  
  urls.forEach(url => {
    try {
      const domain = new URL(url).hostname.replace(/^www\./, '');
      allDomains.push(domain);
      
      if (domain === userDomain) {
        userCitationCount++;
      }
    } catch (e) {
      // Invalid URL, skip
    }
  });
  
  // Also check for domain mentions without full URL
  const domainRegex = new RegExp(`\\b${userDomain.replace('.', '\\.')}\\b`, 'gi');
  const domainMentions = (responseText.match(domainRegex) || []).length;
  userCitationCount += domainMentions;
  
  // Get unique competitor domains (exclude common sites)
  const excludedDomains = ['wikipedia.org', 'youtube.com', 'facebook.com', 'twitter.com', 'linkedin.com'];
  const competitors = [...new Set(allDomains)]
    .filter(d => d !== userDomain && !excludedDomains.includes(d));
  
  return {
    cited: userCitationCount > 0,
    citationCount: userCitationCount,
    competitors,
    totalCitations: allDomains.length,
  };
}

export function calculateScore(citations: any): number {
  if (!citations.cited) return 0;
  
  // Simple scoring: 50 base points for being cited + 25 per citation
  let score = 50;
  score += citations.citationCount * 25;
  
  // Cap at 100
  return Math.min(100, score);
}
```

### **Step 3: Storage (JSON File - Simple)**

```typescript
// lib/geoStorage.ts

import fs from 'fs/promises';
import path from 'path';

const GEO_DATA_FILE = path.join(process.cwd(), 'data', 'geo-tracking.json');

export async function saveQuery(siteUrl: string, query: string) {
  const data = await loadGeoData();
  
  if (!data.queries[siteUrl]) {
    data.queries[siteUrl] = [];
  }
  
  const exists = data.queries[siteUrl].find(q => q.query === query);
  if (!exists) {
    data.queries[siteUrl].push({
      id: Date.now().toString(),
      query,
      createdAt: new Date().toISOString(),
      active: true,
    });
  }
  
  await saveGeoData(data);
}

export async function saveTestResult(siteUrl: string, queryId: string, result: any) {
  const data = await loadGeoData();
  
  if (!data.results[siteUrl]) {
    data.results[siteUrl] = {};
  }
  if (!data.results[siteUrl][queryId]) {
    data.results[siteUrl][queryId] = [];
  }
  
  data.results[siteUrl][queryId].push({
    ...result,
    testedAt: new Date().toISOString(),
  });
  
  await saveGeoData(data);
}

async function loadGeoData() {
  try {
    const content = await fs.readFile(GEO_DATA_FILE, 'utf-8');
    return JSON.parse(content);
  } catch (error) {
    return { queries: {}, results: {} };
  }
}

async function saveGeoData(data: any) {
  await fs.writeFile(GEO_DATA_FILE, JSON.stringify(data, null, 2));
}
```

---

## ğŸ“‹ Implementation Checklist

### Week 1: Backend Foundation
- [ ] Install OpenAI and Anthropic packages
- [ ] Create `/api/geo/test-query/route.ts`
- [ ] Implement `extractCitations()` function
- [ ] Test ChatGPT integration
- [ ] Test Claude integration
- [ ] Create JSON storage structure
- [ ] Create `/api/geo/queries/route.ts` (CRUD)

### Week 2: Frontend UI
- [ ] Create `app/dashboard/geo/page.tsx`
- [ ] Add "GEO" tab to dashboard layout
- [ ] Create `GEOQueryTester` component
- [ ] Create `GEOResultsTable` component
- [ ] Create `GEOMetricsCards` component
- [ ] Add loading states
- [ ] Add error handling

### Week 3: Integration & Polish
- [ ] Create `/api/geo/import-gsc/route.ts`
- [ ] Implement GSC query import with filtering
- [ ] Add "Import from Search Console" button
- [ ] Implement re-test functionality
- [ ] Add historical tracking view
- [ ] Polish UI/UX
- [ ] Test with real queries
- [ ] Fix bugs

---

## ğŸ¯ Success Criteria for MVP

**Technical:**
- âœ… User can test any query on ChatGPT and Claude
- âœ… Citation detection works with >70% accuracy
- âœ… Results display within 10 seconds
- âœ… Can save and re-test queries

**User Value:**
- âœ… User discovers if they're cited in AI responses
- âœ… User sees which competitors are cited instead
- âœ… User can track 10-20 important queries
- âœ… User gets basic visibility metrics

**Performance:**
- âœ… Test execution: <10 seconds
- âœ… Dashboard load: <2 seconds
- âœ… Monthly API cost: <$20

---

## ğŸ’­ Future Enhancements (Phase 2+)

### **If MVP is Successful, Add:**

1. **Automated Testing**
   - Cron job to re-test queries weekly
   - Email notifications when status changes

2. **Perplexity Integration**
   - Add Perplexity API
   - Native citation support (easier than ChatGPT!)

3. **Basic Fan-Out Detection**
   - Only for ChatGPT (easiest to extract)
   - Show detected fan-out queries
   - Don't re-execute them yet (too expensive)

4. **Better Analytics:**
   - Trend charts over time
   - Citation rate per query type
   - Competitor share of voice

5. **Query Suggestions:**
   - Analyze GSC queries automatically
   - Suggest which ones to track
   - Auto-categorize by type

6. **Export/Reporting:**
   - PDF reports
   - CSV export
   - Share results with team

---

## ğŸš¦ Go/No-Go Decision Points

### **After Week 1 (Backend):**
**Question:** Do ChatGPT and Claude integrations work reliably?
- âœ… YES â†’ Continue to Week 2
- âŒ NO â†’ Debug or pivot to manual screenshot upload

### **After Week 2 (Frontend):**
**Question:** Is the UI usable and does citation detection work?
- âœ… YES â†’ Continue to Week 3
- âŒ NO â†’ Improve accuracy or simplify further

### **After Week 3 (MVP Complete):**
**Question:** Do 3-5 test users find this valuable?
- âœ… YES â†’ Launch to all users, plan Phase 2
- âŒ NO â†’ Iterate on core value prop or deprioritize

---

## ğŸ¯ Launch Strategy

### **Soft Launch:**
1. Beta test with 3-5 power users
2. Get feedback on:
   - Is citation detection accurate?
   - Is this information valuable?
   - What features are missing?
3. Iterate based on feedback

### **Full Launch:**
1. Add to dashboard as new tab
2. Announce via email/in-app notification
3. Create help documentation
4. Offer free trial of 100 tests
5. Optional: Charge for unlimited testing

---

## ğŸ’¡ Key Insights for Success

### **What Makes This Realistic:**

1. **Start Small:** Only 2 engines, manual testing
2. **Use Existing Infrastructure:** Next.js, JSON storage, same auth
3. **Simple Citation Detection:** Regex-based, good enough for MVP
4. **Low Cost:** ~$10-20/month in API fees
5. **Fast Development:** Reuse patterns from existing dashboard
6. **Clear Value:** Users immediately see if they're cited

### **What Makes This Valuable:**

1. **Unique:** No other tool in market does this (yet)
2. **Actionable:** Shows concrete gaps (not cited? improve content!)
3. **Competitive Intel:** See what competitors are cited
4. **Future-Proof:** AI search is the future, be early

### **What Can Wait:**

- Fan-out tracking (complex, expensive)
- Automated scheduling (nice-to-have)
- Advanced analytics (premature)
- Multiple languages (can add incrementally)

---

## ğŸ“ Next Steps

### **To Start Implementation:**

1. **Get API Keys:**
   - OpenAI API key (https://platform.openai.com/api-keys)
   - Anthropic API key (https://console.anthropic.com/)

2. **Create Test Account:**
   - Use small budget ($5-10) for testing
   - Test with 5-10 queries first

3. **Validate Concept:**
   - Manually test "hvad er tommer til cm" in ChatGPT
   - See if omregne.dk is mentioned
   - Verify this is worth building

4. **Build MVP:**
   - Follow 3-week timeline above
   - Ship something working, not perfect
   - Get user feedback

5. **Iterate:**
   - Add features based on actual usage
   - Don't over-engineer before validation

---

## ğŸŠ Why This Will Work

### **Advantages:**

1. **Simple to Understand:** "See if AI cites you" - clear value prop
2. **Fast to Build:** Reuse existing patterns, simple APIs
3. **Low Cost:** $10-20/month is nothing for business value
4. **Immediate Value:** First test shows results in 10 seconds
5. **Scalable:** Easy to add more engines/features later

### **Risks & Mitigations:**

**Risk 1:** Citation detection accuracy
- **Mitigation:** Start with simple regex, improve with user feedback

**Risk 2:** API costs if many users
- **Mitigation:** User budgets, rate limiting, caching

**Risk 3:** Users don't find value
- **Mitigation:** Beta test first, iterate based on feedback

**Risk 4:** AI APIs change
- **Mitigation:** Abstract API calls, easy to update

---

## ğŸ“Š Comparison: Original Plan vs Realistic MVP

| Feature | Original Plan | Realistic MVP | Notes |
|---------|---------------|---------------|-------|
| AI Engines | 4+ (ChatGPT, Claude, Perplexity, Gemini) | 2 (ChatGPT, Claude) | Add more later |
| Fan-out Tracking | âœ… Full tracking | âŒ Not included | Too complex for MVP |
| Query Import | GSC + expansion + manual | GSC + manual only | Keep simple |
| Execution | Automated daily/weekly | Manual on-demand | Automate in Phase 2 |
| Citation Extraction | Multi-method + LLM | Simple regex | Good enough for MVP |
| Analytics | Advanced with trends | Basic counts | Add depth later |
| Database | PostgreSQL with 8 tables | JSON file, 3 structures | Upgrade if needed |
| Development Time | 16-24 weeks | 2-3 weeks | 8x faster! |
| Monthly Cost | $600+ | $10-20 | 30x cheaper! |

---

## ğŸš€ The Bottom Line

**Original Plan:** Amazing vision, but too ambitious for first version.

**Realistic MVP:** Delivers 80% of the value with 20% of the complexity.

### **What You Get:**

âœ… Test queries against ChatGPT and Claude
âœ… See if omregne.dk is cited
âœ… Track competitors
âœ… Save queries for tracking over time
âœ… Import top GSC queries
âœ… Simple metrics and trends

### **What You DON'T Get (Yet):**

âŒ Fan-out query tracking (too complex)
âŒ Automated daily testing (can add later)
âŒ Perplexity/Gemini (can add later)
âŒ Advanced analytics (start simple)

### **Development Effort:**

- **Original Plan:** 16-24 weeks, $600+/month
- **Realistic MVP:** 2-3 weeks, $10-20/month

### **Recommendation:**

**Build the MVP first.** If users love it and use it actively, THEN invest in:
- Fan-out tracking
- More AI engines
- Automation
- Advanced analytics

Don't build everything upfront. Ship fast, learn, iterate! ğŸš€

---

## ğŸ“ Ready to Build?

**Switch to agent mode and say:** 
"Implementer GEO tracking MVP"

I'll build:
1. âœ… GEO tab UI
2. âœ… Query tester component
3. âœ… ChatGPT + Claude integration
4. âœ… Citation extraction
5. âœ… Results storage and display

**Timeline:** 2-3 weeks part-time, or 1 week full-time.

Let's revolutionize how people track AI visibility! ğŸ¯


